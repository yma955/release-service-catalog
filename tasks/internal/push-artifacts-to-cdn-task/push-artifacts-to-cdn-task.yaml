---
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: push-artifacts-to-cdn-task
  annotations:
    tekton.dev/pipelines.minVersion: "0.12.1"
    tekton.dev/tags: release
spec:
  description: |-
    Tekton task to push artifacts to the Customer Portal using Pulp and optionally to
    the Developer Portal using exodus-rsync with optional signing. It uses logic based
    on the snapshot data to determine which targets to publish to: if components contain
    both `staged` and `contentGateway` data, artifacts are pushed to the Customer Portal
    (Pulp) and to CGW; if components contain `staged` data only, artifacts are
    pushed to the Customer Portal (Pulp); if components contain `contentGateway`
    data only, artifacts are pushed to the Developer Portal (exodus-rsync) and CGW.
  params:
    - name: snapshot_json
      type: string
      description: String containing a JSON representation of the snapshot spec
    - name: concurrentLimit
      type: string
      description: The maximum number of images to be pulled at once
      default: 3
    # signing params start here
    - name: author
      type: string
      description: Author taken from Release to be used for checksum signing
    - name: signingKeyName
      type: string
      description: Signing key name to be used for checksum signing
    - name: quayURL
      type: string
      description: Quay URL of the repo where content will be shared between tasks
      default: quay.io/konflux-artifacts
    - name: quaySecret
      type: string
      description: Secret to interact with Quay
      default: quay-credentials
    - name: windowsCredentials
      type: string
      description: Secret to interact with the Windows signing host
      default: windows-credentials
    - name: windowsSSHKey
      type: string
      description: Secret containing SSH private key for the Windows signing host
      default: windows-ssh-key
    - name: macHostCredentials
      type: string
      description: Secret to interact with the Mac signing host
      default: mac-host-credentials
    - name: macSigningCredentials
      type: string
      description: Secret to interact with the Mac signing utils
      default: mac-signing-credentials
    - name: macSSHKey
      type: string
      description: Secret containing SSH private key for the Mac signing host
      default: mac-ssh-key
    - name: checksumCredentials
      type: string
      description: Secret containing the keytab, user, host, and fingerprint for the checksum host
      default: checksum-credentials
    - name: kerberosRealm
      type: string
      description: Kerberos realm for the checksum host
      default: IPA.REDHAT.COM
    # cdn params start here
    - name: exodusGwSecret
      type: string
      description: Env specific secret containing the Exodus Gateway configs
    - name: exodusGwEnv
      type: string
      description: Environment to use in the Exodus Gateway. Options are [live, pre]
    - name: pulpSecret
      type: string
      description: Env specific secret containing the rhsm-pulp credentials
    - name: udcacheSecret
      type: string
      description: Env specific secret containing the udcache credentials
    - name: cgwHostname
      type: string
      description: Env specific hostname for content gateway
    - name: cgwSecret
      type: string
      description: Env specific secret containing the content gateway credentials
    - name: caTrustConfigMapName
      type: string
      description: The name of the ConfigMap to read CA bundle data from
      default: trusted-ca
    - name: caTrustConfigMapKey
      type: string
      description: The name of the key in the ConfigMap that contains the CA bundle data
      default: ca-bundle.crt
    - name: certExpirationWarnDays
      type: string
      description: Number of days before expiration to warn about certificate expiration
      default: "7"
  results:
    - name: result
      description: Success if the task succeeds, the error otherwise
    - name: publishedFiles
      description: List of published files
  volumes:
    - name: shared-dir
      emptyDir: {}
    - name: mac-ssh-key-vol
      secret:
        secretName: mac-ssh-key
        defaultMode: 0444
    - name: windows-ssh-key-vol
      secret:
        secretName: windows-ssh-key
        defaultMode: 0444
    - name: checksum-credentials-vol
      secret:
        secretName: $(params.checksumCredentials)
        defaultMode: 0444
    - name: redhat-workloads-token
      secret:
        secretName: redhat-workloads-token
        defaultMode: 0444
    - name: quay-secret
      secret:
        secretName: $(params.quaySecret)
        defaultMode: 0444
    - name: mac-host-credentials
      secret:
        secretName: $(params.macHostCredentials)
        defaultMode: 0444
    - name: mac-signing-credentials
      secret:
        secretName: $(params.macSigningCredentials)
        defaultMode: 0444
    - name: windows-credentials
      secret:
        secretName: $(params.windowsCredentials)
        defaultMode: 0444
    - name: exodus-gw-secret
      secret:
        secretName: $(params.exodusGwSecret)
        defaultMode: 0444
    - name: pulp-secret
      secret:
        secretName: $(params.pulpSecret)
        defaultMode: 0444
    - name: udcache-secret
      secret:
        secretName: $(params.udcacheSecret)
        defaultMode: 0444
    - name: cgw-secret
      secret:
        secretName: $(params.cgwSecret)
        defaultMode: 0444
    - name: trusted-ca
      configMap:
        name: $(params.caTrustConfigMapName)
        items:
          - key: $(params.caTrustConfigMapKey)
            path: ca-bundle.crt
        optional: true
  stepTemplate:
    volumeMounts:
      - name: trusted-ca
        mountPath: /mnt/trusted-ca
        readOnly: true

  steps:
    - name: extract-artifacts
      image: quay.io/konflux-ci/release-service-utils@sha256:454e457cdadd5892b31fe2b7d4911b3d3a761f5705bb0cd30f864be0a597594b
      securityContext:
        runAsUser: 1001
      computeResources:
        limits:
          memory: 512Mi
        requests:
          memory: 512Mi
          cpu: 250m
      volumeMounts:
        - name: shared-dir
          mountPath: /shared
        - name: redhat-workloads-token
          mountPath: /mnt/redhat-workloads-token
      env:
        - name: "SNAPSHOT_JSON"
          value: "$(params.snapshot_json)"
      script: |
        #!/usr/bin/env bash
        set -e

        DOCKER_CONFIG_JSON="$(cat /mnt/redhat-workloads-token/.dockerconfigjson)"

        set -x

        STDERR_FILE=/tmp/stderr.txt

        exitfunc() {
            local err=$1
            local line=$2
            local command="$3"
            if [ "$err" -eq 0 ] ; then
                echo -n "Success" > "$(results.result.path)"
            else
                echo "$0: ERROR '$command' failed at line $line - exited with status $err" \
                  > "$(results.result.path)"
                if [ -f "$STDERR_FILE" ] ; then
                    # save output to the task result excluding the trace log
                    grep -v "^\+" "$STDERR_FILE" | tail -c 512 | tee -a "$(results.result.path)"
                fi
            fi
            exit 0 # exit the script cleanly as there is no point in proceeding past an error or exit call
        }
        # due to set -e, this catches all EXIT and ERR calls and the task should never fail with nonzero exit code
        trap 'exitfunc $? $LINENO "$BASH_COMMAND"' EXIT

        mkdir -p ~/.docker
        set +x
        # Quotes are added to the secret so it applies in k8s nicely. But now we have to remove them
        echo "$DOCKER_CONFIG_JSON" | sed -r 's/(^|\})[^{}]+(\{|$)/\1\2/g' > ~/.docker/config.json
        set -x

        CONTENT_DIR="/shared/artifacts"
        export CONTENT_DIR
        mkdir -p "$CONTENT_DIR"

        process_component() { # Expected arguments: [component json] [component index (0-based)]
            COMPONENT=$1
            COMPONENT_INDEX=$2
            COMPONENT_NUM=$((COMPONENT_INDEX + 1))  # 1-based for error messages
            # Use component.name for directory isolation
            COMPONENT_NAME=$(jq -er '.name' <<< "${COMPONENT}") \
                  || (echo "Component ${COMPONENT_NUM} is missing 'name'" >&2 && exit 1 )
            
            # Check if component has files or staged.files - skip if neither exists
            NUM_FILES=$(jq '.files | length // 0' <<< "${COMPONENT}")
            NUM_STAGED_FILES=$(jq '.staged.files | length // 0' <<< "${COMPONENT}")
            if [ "$NUM_FILES" -eq 0 ] && [ "$NUM_STAGED_FILES" -eq 0 ]; then
              echo "Skipping component '${COMPONENT_NAME}' - no files defined (not a binary artifact component)"
              return 0
            fi
            
            PULLSPEC=$(jq -er '.containerImage' <<< "${COMPONENT}") \
                  || (echo "Component ${COMPONENT_NAME} is missing 'containerImage'" >&2 && exit 1 )
            DESTINATION="${CONTENT_DIR}/${COMPONENT_NAME}"
            echo "Extracting component '${COMPONENT_NAME}' to: $DESTINATION"

            mkdir -p "${DESTINATION}"

            # Extract binaries from container image using skopeo (like extract-binaries-from-image task)
            TMP_DIR=$(mktemp -d)

            # Create auth file for skopeo
            AUTH_FILE=$(mktemp)
            select-oci-auth "${PULLSPEC}" > "$AUTH_FILE"

            # Use skopeo to copy container manifest and layers
            skopeo copy --retry-times 3 --authfile "$AUTH_FILE" docker://"$PULLSPEC" dir:"$TMP_DIR"

            cd "$TMP_DIR"

            # Build list of specific files to extract from files[] and staged.files[]
            # We only want files explicitly listed in the RPA, not everything in the container
            WANTED_FILES=()
            EXTRACT_DIRS=()

            # Check .files[].source paths
            NUM_FILES=$(jq '.files | length // 0' <<< "${COMPONENT}")
            for ((i = 0; i < NUM_FILES; i++)); do
                SOURCE_PATH=$(jq -r --arg i "$i" '.files[$i|tonumber].source // empty' <<< "${COMPONENT}")
                if [ -n "$SOURCE_PATH" ]; then
                    # Store the full path (without leading /) for extraction
                    WANTED_FILES+=("${SOURCE_PATH#/}")
                    # Extract directory part for tar extraction
                    DIR_PATH=$(dirname "$SOURCE_PATH" | sed 's|^/||')
                    if [ "$DIR_PATH" != "." ] && [ -n "$DIR_PATH" ]; then
                        EXTRACT_DIRS+=("$DIR_PATH")
                    fi
                fi
            done

            # Check .staged.files[].source paths
            NUM_STAGED_FILES=$(jq '.staged.files | length // 0' <<< "${COMPONENT}")
            for ((i = 0; i < NUM_STAGED_FILES; i++)); do
                SOURCE_PATH=$(jq -r --arg i "$i" '.staged.files[$i|tonumber].source // empty' <<< "${COMPONENT}")
                if [ -n "$SOURCE_PATH" ]; then
                    # Store the full path (without leading /) for extraction
                    WANTED_FILES+=("${SOURCE_PATH#/}")
                    # Extract directory part for tar extraction
                    DIR_PATH=$(dirname "$SOURCE_PATH" | sed 's|^/||')
                    if [ "$DIR_PATH" != "." ] && [ -n "$DIR_PATH" ]; then
                        EXTRACT_DIRS+=("$DIR_PATH")
                    fi
                fi
            done

            # Default to "releases" if no source directories found
            if [ ${#EXTRACT_DIRS[@]} -eq 0 ]; then
                EXTRACT_DIRS=("releases")
            fi

            # Remove duplicates
            mapfile -t UNIQUE_DIRS < <(printf "%s\n" "${EXTRACT_DIRS[@]}" | sort -u)
            mapfile -t UNIQUE_FILES < <(printf "%s\n" "${WANTED_FILES[@]}" | sort -u)

            echo "Files to extract from RPA: ${UNIQUE_FILES[*]}"

            # Extract the directories from container layers
            for IMAGE_PATH in "${UNIQUE_DIRS[@]}"; do
                echo "Looking for directory: $IMAGE_PATH"
                for DIGEST in $(jq -r ".layers[].digest" manifest.json); do
                    FILE=${DIGEST#sha256:}
                    # Check if the archive contains the target dir
                    if tar -tf "$FILE" | grep -q "^$IMAGE_PATH/"; then
                        echo "Extracting $IMAGE_PATH/ from $FILE..."
                        tar -xzvf "$FILE" "$IMAGE_PATH"
                    else
                        echo "skipping $FILE. It doesn't contain the $IMAGE_PATH dir"
                    fi
                done
            done

            # Copy ONLY files explicitly listed in the RPA (not all files from the directory)
            for wanted_file in "${UNIQUE_FILES[@]}"; do
                if [ -f "$wanted_file" ]; then
                    cp "$wanted_file" "$DESTINATION"/
                else
                    echo "Warning: Expected file not found in container: $wanted_file"
                fi
            done

            # Clean up temporary directory
            rm -rf "$TMP_DIR"

        }

        RUNNING_JOBS="\j" # Bash parameter for number of jobs currently running
        NUM_COMPONENTS=$(jq '.components | length' <<< "$SNAPSHOT_JSON")

        # Pull each component in parallel
        for ((i = 0; i < NUM_COMPONENTS; i++)) ; do
            COMPONENT=$(jq -c --arg i "$i" '.components[$i|tonumber]' <<< "$SNAPSHOT_JSON")
            # Limit batch size to concurrent limit
            while (( ${RUNNING_JOBS@P} >= $(params.concurrentLimit) )); do
                wait -n
            done
            process_component "$COMPONENT" "$i" 2> "$STDERR_FILE" &
        done

        # Wait for remaining processes to finish
        while (( ${RUNNING_JOBS@P} > 0 )); do
            wait -n
        done

        # Create flag files based on OS types specified in the RPA mapping (files array)
        # This ensures we only process OS types that are configured for release, not everything in the container
        FILES_QUERY='(.files[]?, .staged.files[]?)'
        for ((i = 0; i < NUM_COMPONENTS; i++)); do
          COMPONENT=$(jq -c --arg i "$i" '.components[$i|tonumber]' <<< "$SNAPSHOT_JSON")
          COMPONENT_NAME=$(jq -r '.name' <<< "$COMPONENT")
          component_dir="$CONTENT_DIR/$COMPONENT_NAME"
          
          echo "Checking configured OS types for component: $COMPONENT_NAME"
          
          # Check for macOS(darwin) in both .files[] and .staged.files[]
          if jq -e "$FILES_QUERY | select(.os == \"darwin\")" <<< "$COMPONENT" >/dev/null 2>&1 || \
             jq -e "$FILES_QUERY | select((.source // \"\") | contains(\"darwin\"))" \
               <<< "$COMPONENT" >/dev/null 2>&1 || \
             jq -e "$FILES_QUERY | select((.filename // \"\") | contains(\"darwin\"))" \
               <<< "$COMPONENT" >/dev/null 2>&1; then
            touch "$component_dir/has_mac"
            echo "  - macOS content detected"
          fi
          # Check for Windows in both .files[] and .staged.files[]
          if jq -e "$FILES_QUERY | select(.os == \"windows\")" <<< "$COMPONENT" >/dev/null 2>&1 || \
             jq -e "$FILES_QUERY | select((.source // \"\") | contains(\"windows\"))" \
               <<< "$COMPONENT" >/dev/null 2>&1 || \
             jq -e "$FILES_QUERY | select((.filename // \"\") | contains(\"windows\"))" \
               <<< "$COMPONENT" >/dev/null 2>&1; then
            touch "$component_dir/has_windows"
            echo "  - Windows content detected"
          fi
          # Check for Linux in both .files[] and .staged.files[]
          if jq -e "$FILES_QUERY | select(.os == \"linux\")" <<< "$COMPONENT" >/dev/null 2>&1 || \
             jq -e "$FILES_QUERY | select((.source // \"\") | contains(\"linux\"))" \
               <<< "$COMPONENT" >/dev/null 2>&1 || \
             jq -e "$FILES_QUERY | select((.filename // \"\") | contains(\"linux\"))" \
               <<< "$COMPONENT" >/dev/null 2>&1; then
            touch "$component_dir/has_linux"
            echo "  - Linux content detected"
          fi
        done
    - name: push-unsigned-using-oras
      image: quay.io/konflux-ci/release-service-utils@sha256:454e457cdadd5892b31fe2b7d4911b3d3a761f5705bb0cd30f864be0a597594b
      computeResources:
        limits:
          memory: 512Mi
        requests:
          memory: 512Mi
          cpu: 250m
      volumeMounts:
        - name: shared-dir
          mountPath: /shared
        - name: quay-secret
          mountPath: /mnt/quaySecret
      env:
        - name: SNAPSHOT_JSON
          value: "$(params.snapshot_json)"
      script: |
        #!/usr/bin/env bash
        set -eu

        QUAY_USER="$(cat "/mnt/quaySecret/username")"
        QUAY_PASS="$(cat "/mnt/quaySecret/password")"

        set -x

        STDERR_FILE=/tmp/stderr.txt

        # Check if the previous step finished successfully. If not, stop here.
        if [ "$(cat "$(results.result.path)")" != "Success" ]; then
          echo "Previous step failed. Exiting..."
          exit 0
        fi

        exitfunc() {
            local err=$1
            local line=$2
            local command="$3"
            if [ "$err" -eq 0 ] ; then
                echo -n "Success" > "$(results.result.path)"
            else
                echo "$0: ERROR '$command' failed at line $line - exited with status $err" \
                  > "$(results.result.path)"
                if [ -f "$STDERR_FILE" ] ; then
                    # save output to the task result excluding the trace log
                    grep -v "^\+" "$STDERR_FILE" | tail -c 512 | tee -a "$(results.result.path)"
                fi
            fi
            exit 0 # exit the script cleanly as there is no point in proceeding past an error or exit call
        }
        # due to set -e, this catches all EXIT and ERR calls and the task should never fail with nonzero exit code
        trap 'exitfunc $? $LINENO "$BASH_COMMAND"' EXIT

        CONTENT_DIR=/shared/artifacts

        echo "Logging into Quay..."
        set +x
        oras login quay.io -u "${QUAY_USER}" -p "${QUAY_PASS}" > /dev/null 2>&1
        set -x

        # Process each component separately
        NUM_COMPONENTS=$(jq '.components | length' <<< "$SNAPSHOT_JSON")
        for ((c = 0; c < NUM_COMPONENTS; c++)); do
          COMPONENT=$(jq -c --arg i "$c" '.components[$i|tonumber]' <<< "$SNAPSHOT_JSON")
          COMPONENT_NAME=$(jq -r '.name' <<< "$COMPONENT")
          
          echo "Processing component: $COMPONENT_NAME"
          
          # Skip components without files or staged.files (not binary artifact components)
          NUM_FILES=$(jq '.files | length // 0' <<< "$COMPONENT")
          NUM_STAGED_FILES=$(jq '.staged.files | length // 0' <<< "$COMPONENT")
          if [ "$NUM_FILES" -eq 0 ] && [ "$NUM_STAGED_FILES" -eq 0 ]; then
            echo "Skipping component '$COMPONENT_NAME' - no files or staged.files defined"
            continue
          fi
          
          COMPONENT_DIR="$CONTENT_DIR/$COMPONENT_NAME"
          UNSIGNED_DIR="$COMPONENT_DIR/unsigned"
          MAC_CONTENT="$UNSIGNED_DIR/macos"
          WINDOWS_CONTENT="$UNSIGNED_DIR/windows"
          LINUX_CONTENT="$COMPONENT_DIR/linux"

          # Create directories only for OS types that have content (based on flag files from extract step)
          [ -f "$COMPONENT_DIR/has_mac" ] && mkdir -p "$MAC_CONTENT"
          [ -f "$COMPONENT_DIR/has_windows" ] && mkdir -p "$WINDOWS_CONTENT"
          [ -f "$COMPONENT_DIR/has_linux" ] && mkdir -p "$LINUX_CONTENT"

          # Organize binaries into os/arch/ structure based on FILE entries
          # This ensures each binary is placed in the correct location regardless of its name
          # Process files[] array - unpack each archive and organize by os/arch
          NUM_FILES=$(jq '.files | length // 0' <<< "$COMPONENT")
          for ((i = 0; i < NUM_FILES; i++)); do
            FILE=$(jq -c --arg i "$i" '.files[$i|tonumber]' <<< "$COMPONENT")
            SOURCE=$(jq -er '.source' <<< "$FILE")
            OS=$(jq -er '.os' <<< "$FILE")
            ARCH=$(jq -er '.arch' <<< "$FILE")
            
            # Determine OS directory name and target location
            case "$OS" in
              (darwin) 
                OS_DIR="macos"
                TARGET_DIR="$UNSIGNED_DIR/$OS_DIR/$ARCH"
                ;;
              (linux) 
                OS_DIR="linux"
                TARGET_DIR="$COMPONENT_DIR/$OS_DIR/$ARCH"
                ;;
              (windows) 
                OS_DIR="windows"
                TARGET_DIR="$UNSIGNED_DIR/$OS_DIR/$ARCH"
                ;;
            esac
            
            # Extract archive filename from source path
            ARCHIVE_NAME=$(basename "$SOURCE")
            ARCHIVE_PATH="$COMPONENT_DIR/$ARCHIVE_NAME"
            
            if [ ! -f "$ARCHIVE_PATH" ]; then
              echo "  Warning: Archive not found: $ARCHIVE_PATH" >&2
              continue
            fi
            
            # Create target directory
            mkdir -p "$TARGET_DIR"
            
            # Unpack archive to target directory (only .tar.gz supported)
            # Preserve nested directory structure from original archive
            tar -xzf "$ARCHIVE_PATH" -C "$TARGET_DIR"
            
            # Remove the archive after successful extraction
            rm -f "$ARCHIVE_PATH"
          done
          
          # Process staged.files[] array (same logic)
          NUM_STAGED_FILES=$(jq '.staged.files | length // 0' <<< "$COMPONENT")
          for ((i = 0; i < NUM_STAGED_FILES; i++)); do
            FILE=$(jq -c --arg i "$i" '.staged.files[$i|tonumber]' <<< "$COMPONENT")
            SOURCE=$(jq -er '.source' <<< "$FILE")
            OS=$(jq -er '.os' <<< "$FILE")
            ARCH=$(jq -er '.arch' <<< "$FILE")
            
            case "$OS" in
              (darwin) 
                OS_DIR="macos"
                TARGET_DIR="$UNSIGNED_DIR/$OS_DIR/$ARCH"
                ;;
              (linux) 
                OS_DIR="linux"
                TARGET_DIR="$COMPONENT_DIR/$OS_DIR/$ARCH"
                ;;
              (windows) 
                OS_DIR="windows"
                TARGET_DIR="$UNSIGNED_DIR/$OS_DIR/$ARCH"
                ;;
            esac
            
            ARCHIVE_NAME=$(basename "$SOURCE")
            ARCHIVE_PATH="$COMPONENT_DIR/$ARCHIVE_NAME"
            
            if [ ! -f "$ARCHIVE_PATH" ]; then
              continue
            fi
            
            mkdir -p "$TARGET_DIR"
            
            # Unpack archive to target directory (only .tar.gz supported)
            # Preserve nested directory structure from original archive
            tar -xzf "$ARCHIVE_PATH" -C "$TARGET_DIR"
            
            rm -f "$ARCHIVE_PATH"
          done

          # Push unsigned Mac content
          if [ -f "$COMPONENT_DIR/has_mac" ]; then
            pushd "$UNSIGNED_DIR" > /dev/null
            echo "Pushing unsigned Macos content for $COMPONENT_NAME to $(params.quayURL)..."
            output=$(oras push "$(params.quayURL)/${COMPONENT_NAME}/unsigned" macos)
            mac_digest=$(echo "$output" | grep 'Digest:' | awk '{print $2}')
            echo "Digest for $COMPONENT_NAME mac content: $mac_digest"
            echo -n "$mac_digest" > "$COMPONENT_DIR/unsigned_mac_digest.txt"
            popd > /dev/null
          else
            echo "No macOS content for $COMPONENT_NAME, skipping unsigned push..."
          fi

          # Push unsigned Windows content
          if [ -f "$COMPONENT_DIR/has_windows" ]; then
            pushd "$UNSIGNED_DIR" > /dev/null
            echo "Pushing unsigned Windows content for $COMPONENT_NAME to $(params.quayURL)..."
            output=$(oras push "$(params.quayURL)/${COMPONENT_NAME}/unsigned" windows)
            windows_digest=$(echo "$output" | grep 'Digest:' | awk '{print $2}')
            echo "Digest for $COMPONENT_NAME windows content: $windows_digest"
            echo -n "$windows_digest" > "$COMPONENT_DIR/unsigned_windows_digest.txt"
            popd > /dev/null
          else
            echo "No Windows content for $COMPONENT_NAME, skipping unsigned push..."
          fi
        done
    - name: sign-mac-binaries
      image: quay.io/konflux-ci/release-service-utils@sha256:454e457cdadd5892b31fe2b7d4911b3d3a761f5705bb0cd30f864be0a597594b
      computeResources:
        limits:
          memory: 512Mi
        requests:
          memory: 512Mi
          cpu: 250m
      volumeMounts:
        - name: shared-dir
          mountPath: /shared
        - name: mac-ssh-key-vol
          mountPath: "/mnt/secrets"
        - name: quay-secret
          mountPath: /mnt/quaySecret
        - name: mac-host-credentials
          mountPath: /mnt/macHostCredentials
        - name: mac-signing-credentials
          mountPath: /mnt/macSigningCredentials
      env:
        - name: QUAY_URL
          value: $(params.quayURL)
        - name: PIPELINE_UID
          value: $(context.taskRun.uid)
        - name: SNAPSHOT_JSON
          value: "$(params.snapshot_json)"
      script: |
        #!/usr/bin/env bash
        set -eu

        MAC_USER="$(cat /mnt/macHostCredentials/username)"
        MAC_HOST="$(cat /mnt/macHostCredentials/host)"
        KEYCHAIN_PASSWORD="$(cat /mnt/macSigningCredentials/keychain_password)"
        SIGNING_IDENTITY="$(cat /mnt/macSigningCredentials/signing_identity)"
        APPLE_ID="$(cat /mnt/macSigningCredentials/apple_id)"
        TEAM_ID="$(cat /mnt/macSigningCredentials/team_id)"
        APP_SPECIFIC_PASSWORD="$(cat /mnt/macSigningCredentials/app_specific_password)"
        QUAY_USER="$(cat /mnt/quaySecret/username)"
        QUAY_PASS="$(cat /mnt/quaySecret/password)"

        set -x

        STDERR_FILE=/tmp/stderr.txt

        # Check if the previous step finished successfully. If not, stop here.
        if [ "$(cat "$(results.result.path)")" != "Success" ]; then
          echo "Previous step failed. Exiting..."
          exit 0
        fi

        exitfunc() {
            local err=$1
            local line=$2
            local command="$3"
            if [ "$err" -eq 0 ] ; then
                echo -n "Success" > "$(results.result.path)"
            else
                echo "$0: ERROR '$command' failed at line $line - exited with status $err" \
                  > "$(results.result.path)"
                if [ -f "$STDERR_FILE" ] ; then
                    # save output to the task result excluding the trace log
                    grep -v "^\+" "$STDERR_FILE" | tail -c 512 | tee -a "$(results.result.path)"
                fi
            fi
            exit 0 # exit the script cleanly as there is no point in proceeding past an error or exit call
        }
        # due to set -e, this catches all EXIT and ERR calls and the task should never fail with nonzero exit code
        trap 'exitfunc $? $LINENO "$BASH_COMMAND"' EXIT

        mkdir -p /tmp/.ssh
        chmod 700 /tmp/.ssh
        cp "/mnt/secrets/mac_id_rsa" /tmp/.ssh/id_rsa
        cp "/mnt/secrets/mac_fingerprint" /tmp/.ssh/known_hosts
        chmod 600 /tmp/.ssh/id_rsa /tmp/.ssh/known_hosts

        SSH_OPTS=(-i /tmp/.ssh/id_rsa -o UserKnownHostsFile=/tmp/.ssh/known_hosts -o IdentitiesOnly=yes)

        CONTENT_DIR=/shared/artifacts

        # Process each component sequentially
        NUM_COMPONENTS=$(jq '.components | length' <<< "$SNAPSHOT_JSON")
        for ((c = 0; c < NUM_COMPONENTS; c++)); do
          COMPONENT=$(jq -c --arg i "$c" '.components[$i|tonumber]' <<< "$SNAPSHOT_JSON")
          COMPONENT_NAME=$(jq -r '.name' <<< "$COMPONENT")
          COMPONENT_DIR="$CONTENT_DIR/$COMPONENT_NAME"

          # Skip if no macOS content for this component
          if [ ! -f "$COMPONENT_DIR/has_mac" ]; then
            echo "No macOS content for component $COMPONENT_NAME, skipping Mac signing..."
            continue
          fi

          echo "Signing Mac binaries for component: $COMPONENT_NAME"

          # Read unsigned digest for this component
          unsigned_digest=$(cat "$COMPONENT_DIR/unsigned_mac_digest.txt")
          mac_signing_script="/tmp/mac_signing_script_$(context.taskRun.uid)_${COMPONENT_NAME}.sh"

          TEMP_DIR="/tmp/$(context.taskRun.uid)_${COMPONENT_NAME}"
          BINARY_PATH="$TEMP_DIR/unsigned"
          ZIP_PATH="$TEMP_DIR/signed_content.zip"
          DIGEST_FILE="$TEMP_DIR/push_digest.txt"

          cat << EOF > "$mac_signing_script"
          #!/bin/bash
          set -eux

          mkdir -p "$TEMP_DIR"
          mkdir -p "$BINARY_PATH"

          cd "$TEMP_DIR"
          set +x
          /usr/local/bin/oras login quay.io -u ${QUAY_USER} -p ${QUAY_PASS}
          set -x
          /usr/local/bin/oras pull $(params.quayURL)/${COMPONENT_NAME}/unsigned@$unsigned_digest -o "$BINARY_PATH"
          # The content is extracted to BINARY_PATH/macos (no nesting since we pull to unsigned, not unsigned/macos)
          CONTENT_DIR_MAC="$BINARY_PATH/macos"

          set +x
          security unlock-keychain -p $KEYCHAIN_PASSWORD login.keychain
          set -x
          echo "Signing files in the \$CONTENT_DIR_MAC directory..."
          find "\$CONTENT_DIR_MAC" -type f | while read file; do
              echo "Signing: \$file"
              if ! xcrun codesign --sign "Developer ID Application: $SIGNING_IDENTITY" \
                  --options runtime --timestamp --force "\$file"; then
                  echo "Failed to sign file: \$file"
                  exit 1
              fi
          done

          cd "$BINARY_PATH"
          zip -r "$ZIP_PATH" macos

          echo "Submitting ZIP file to Apple notary service..."
          set +x
          xcrun notarytool submit "$ZIP_PATH" \
              --wait \
              --apple-id "$APPLE_ID" \
              --team-id "$TEAM_ID" \
              --password "$APP_SPECIFIC_PASSWORD"
          set -x

          SIGNED_TAG="$(context.taskRun.uid)-mac"
          PUSH_OUTPUT=\$(/usr/local/bin/oras push \
            "$QUAY_URL/${COMPONENT_NAME}/signed:\$SIGNED_TAG" macos)
          SIGNED_DIGEST=\$(echo "\$PUSH_OUTPUT" | grep 'Digest:' | awk '{print \$2}')
          echo -n "\$SIGNED_DIGEST" >> "$DIGEST_FILE"
          echo "Process completed successfully."
        EOF

          # Copy the script to the Mac host
          scp "${SSH_OPTS[@]}" "$mac_signing_script" "${MAC_USER}@${MAC_HOST}:${mac_signing_script}"

          # Execute the script on the Mac host
          # shellcheck disable=SC2029
          ssh "${SSH_OPTS[@]}" "${MAC_USER}@${MAC_HOST}" bash "${mac_signing_script}"

          # Copy the signed digest back to the shared volume for this component
          scp "${SSH_OPTS[@]}" "${MAC_USER}@${MAC_HOST}:${TEMP_DIR}/push_digest.txt" \
              "$COMPONENT_DIR/signed_mac_digest.txt"

          # Clean up the Mac host now that we are done with this component
          # shell check complains the variables evaluate on the client side, but we want that here
          # shellcheck disable=SC2029
          ssh "${SSH_OPTS[@]}" "${MAC_USER}@${MAC_HOST}" "rm -rf ${TEMP_DIR} ${mac_signing_script}"
        done
    - name: sign-windows-binaries
      image: quay.io/konflux-ci/release-service-utils@sha256:454e457cdadd5892b31fe2b7d4911b3d3a761f5705bb0cd30f864be0a597594b
      computeResources:
        limits:
          memory: 512Mi
        requests:
          memory: 512Mi
          cpu: 250m
      volumeMounts:
        - name: shared-dir
          mountPath: /shared
        - name: windows-ssh-key-vol
          mountPath: /mnt/secrets
        - name: quay-secret
          mountPath: /mnt/quaySecret
        - name: windows-credentials
          mountPath: /mnt/windowsCredentials
      env:
        - name: SNAPSHOT_JSON
          value: "$(params.snapshot_json)"
      script: |
        #!/usr/bin/env bash
        set -eu

        WINDOWS_USER="$(cat /mnt/windowsCredentials/username)"
        WINDOWS_PORT="$(cat /mnt/windowsCredentials/port)"
        WINDOWS_HOST="$(cat /mnt/windowsCredentials/host)"
        QUAY_USER="$(cat /mnt/quaySecret/username)"
        QUAY_PASS="$(cat /mnt/quaySecret/password)"

        set -x

        STDERR_FILE=/tmp/stderr.txt

        # Check if the previous step finished successfully. If not, stop here.
        if [ "$(cat "$(results.result.path)")" != "Success" ]; then
          echo "Previous step failed. Exiting..."
          exit 0
        fi

        exitfunc() {
            local err=$1
            local line=$2
            local command="$3"
            if [ "$err" -eq 0 ] ; then
                echo -n "Success" > "$(results.result.path)"
            else
                echo "$0: ERROR '$command' failed at line $line - exited with status $err" \
                  > "$(results.result.path)"
                if [ -f "$STDERR_FILE" ] ; then
                    # save output to the task result excluding the trace log
                    grep -v "^\+" "$STDERR_FILE" | tail -c 512 | tee -a "$(results.result.path)"
                fi
            fi
            exit 0 # exit the script cleanly as there is no point in proceeding past an error or exit call
        }
        # due to set -e, this catches all EXIT and ERR calls and the task should never fail with nonzero exit code
        trap 'exitfunc $? $LINENO "$BASH_COMMAND"' EXIT

        mkdir -p /tmp/.ssh
        chmod 700 /tmp/.ssh
        cp "/mnt/secrets/windows_id_rsa" /tmp/.ssh/id_rsa
        cp "/mnt/secrets/windows_fingerprint" /tmp/.ssh/known_hosts
        chmod 600 /tmp/.ssh/known_hosts /tmp/.ssh/id_rsa

        SSH_OPTS=(-i /tmp/.ssh/id_rsa -o UserKnownHostsFile=/tmp/.ssh/known_hosts \
          -o IdentitiesOnly=yes -p "${WINDOWS_PORT}")
        SCP_OPTS=(-i /tmp/.ssh/id_rsa -o UserKnownHostsFile=/tmp/.ssh/known_hosts \
          -o IdentitiesOnly=yes -P "${WINDOWS_PORT}")

        CONTENT_DIR=/shared/artifacts

        # Process each component sequentially
        NUM_COMPONENTS=$(jq '.components | length' <<< "$SNAPSHOT_JSON")
        for ((c = 0; c < NUM_COMPONENTS; c++)); do
          COMPONENT=$(jq -c --arg i "$c" '.components[$i|tonumber]' <<< "$SNAPSHOT_JSON")
          COMPONENT_NAME=$(jq -r '.name' <<< "$COMPONENT")
          COMPONENT_DIR="$CONTENT_DIR/$COMPONENT_NAME"

          # Skip if no Windows content for this component
          if [ ! -f "$COMPONENT_DIR/has_windows" ]; then
            echo "No Windows content for component $COMPONENT_NAME, skipping Windows signing..."
            continue
          fi

          echo "Signing Windows binaries for component: $COMPONENT_NAME"

          # Read unsigned digest for this component
          unsigned_digest=$(cat "$COMPONENT_DIR/unsigned_windows_digest.txt")

          # Create the batch script
          windows_signing_script_file="/tmp/windows_signing_script_file_$(context.taskRun.uid)_${COMPONENT_NAME}.bat"
          WINDOWS_TEMP_DIR="$(context.taskRun.uid)_${COMPONENT_NAME}"
          WIN_TEMP="C:/Users/${WINDOWS_USER}/AppData/Local/Temp"
          WINDOWS_SIGNING_SCRIPT_PATH="${WIN_TEMP}/windows_signing_script_file_${WINDOWS_TEMP_DIR}.bat"
          set +x
          cat << EOF > "$windows_signing_script_file"

          mkdir %TEMP%\\${WINDOWS_TEMP_DIR} && cd /d %TEMP%\\${WINDOWS_TEMP_DIR}
          @echo off
          oras login quay.io -u ${QUAY_USER} -p ${QUAY_PASS}
          @echo on
          oras pull $(params.quayURL)/${COMPONENT_NAME}/unsigned@${unsigned_digest} -o unsigned
          REM The content is extracted to unsigned\windows with os/arch/ subdirectories (e.g., unsigned\windows\amd64\)

          REM Recursively sign all files in unsigned\windows directory tree
          for /r unsigned\windows %%f in (*) do (
            signtool sign /v /n "Red Hat" /fd SHA256 /tr http://timestamp.digicert.com /td SHA256 "%%f"
            if errorlevel 1 (
              echo Signing of %%f failed
              exit /B %ERRORLEVEL%
            )
          )

          REM Recursively verify all signed files
          for /r unsigned\windows %%f in (*) do (
            signtool verify /v /pa "%%f"
            if errorlevel 1 (
              echo Verification of %%f failed
              exit /B %ERRORLEVEL%
            )
          )

          if errorlevel 1 (
            echo Verification of binaries failed
            exit /B %ERRORLEVEL%
          )

          echo [%DATE% %TIME%] Signing of Windows binaries for ${COMPONENT_NAME} completed successfully

          cd unsigned
          oras push $(params.quayURL)/${COMPONENT_NAME}/signed:$(context.taskRun.uid)-windows windows \
          > oras_push_output.txt 2>&1

          for /f "tokens=2,3 delims=: " %%a in ('findstr "Digest:" oras_push_output.txt') do @echo %%a:%%b > digest.txt
        EOF
          set -x
          # shellcheck disable=SC2086
          scp "${SCP_OPTS[@]}" "$windows_signing_script_file" \
          "${WINDOWS_USER}@${WINDOWS_HOST}:${WINDOWS_SIGNING_SCRIPT_PATH}"

          # Execute the script on the Windows host
          # shellcheck disable=SC2029,SC2086
          ssh "${SSH_OPTS[@]}" "${WINDOWS_USER}@${WINDOWS_HOST}" \
            "${WINDOWS_SIGNING_SCRIPT_PATH}"

          # Copy signed digest back to shared volume for this component
          # shellcheck disable=SC2029,SC2086
          WINDOWS_DIGEST_PATH="C:/Users/${WINDOWS_USER}/AppData/Local/Temp/${WINDOWS_TEMP_DIR}/unsigned/digest.txt"
          scp "${SCP_OPTS[@]}" "${WINDOWS_USER}@${WINDOWS_HOST}:${WINDOWS_DIGEST_PATH}" \
          "$COMPONENT_DIR/signed_windows_digest.txt"

          # Remove trailing spaces, carriage returns, newlines
          sed -i 's/[[:space:]]*$//; s/\r//g; :a;N;$!ba;s/\n//g' "$COMPONENT_DIR/signed_windows_digest.txt"

          # Clean up the windows host now that we are done with this component
          WINDOWS_CLEANUP_PATH="C:\\Users\\${WINDOWS_USER}\\AppData\\Local\\Temp\\${WINDOWS_TEMP_DIR}"
          # shellcheck disable=SC2029,SC2086
          ssh "${SSH_OPTS[@]}" "${WINDOWS_USER}@${WINDOWS_HOST}" \
            "Remove-Item -LiteralPath ${WINDOWS_CLEANUP_PATH} -Force -Recurse; \
            Remove-Item -LiteralPath ${WINDOWS_SIGNING_SCRIPT_PATH} -Force"
        done
    - name: generate-checksums
      image: quay.io/konflux-ci/release-service-utils@sha256:454e457cdadd5892b31fe2b7d4911b3d3a761f5705bb0cd30f864be0a597594b
      computeResources:
        limits:
          memory: 512Mi
        requests:
          memory: 512Mi
          cpu: 250m
      volumeMounts:
        - name: shared-dir
          mountPath: /shared
        - name: checksum-credentials-vol
          mountPath: /mnt/checksum_credentials
        - name: quay-secret
          mountPath: /mnt/quaySecret
      env:
        - name: AUTHOR
          value: $(params.author)
        - name: SIGNING_KEY_NAME
          value: $(params.signingKeyName)
        - name: SNAPSHOT_JSON
          value: "$(params.snapshot_json)"
      script: |
        #!/usr/bin/env bash
        set -eu

        QUAY_USER="$(cat /mnt/quaySecret/username)"
        QUAY_PASS="$(cat /mnt/quaySecret/password)"

        set -x

        #---------------------------------------------------------------------------------------
        # This step generates checksums for all of the binaries in the content directory and
        # signs them using the checksum host.
        # The general workflow is that the binaries are extracted from the image(previous task),
        # signed on remote hosts (windows and mac) and then a sha256sum is generated for each
        # binary. The shasums are collected in a sha256sum.txt file which is then transferred to
        # the checksum host for signing with Red Hat's GPG key.
        # The detached signatures are returned to the workspace for inclusion in the later tasks
        # to be pushed to CDN and the Red Hat Developer Portal.
        #---------------------------------------------------------------------------------------

        STDERR_FILE=/tmp/stderr.txt

        # Check if the previous step finished successfully. If not, stop here.
        if [ "$(cat "$(results.result.path)")" != "Success" ]; then
          echo "Previous step failed. Exiting..."
          exit 0
        fi

        exitfunc() {
            local err=$1
            local line=$2
            local command="$3"
            if [ "$err" -eq 0 ] ; then
                echo -n "Success" > "$(results.result.path)"
            else
                echo "$0: ERROR '$command' failed at line $line - exited with status $err" \
                  > "$(results.result.path)"
                if [ -f "$STDERR_FILE" ] ; then
                    # save output to the task result excluding the trace log
                    grep -v "^\+" "$STDERR_FILE" | tail -c 512 | tee -a "$(results.result.path)"
                fi
            fi
            exit 0 # exit the script cleanly as there is no point in proceeding past an error or exit call
        }
        # due to set -e, this catches all EXIT and ERR calls and the task should never fail with nonzero exit code
        trap 'exitfunc $? $LINENO "$BASH_COMMAND"' EXIT

        if [[ ${AUTHOR} == "" ]] ; then
          echo Error: invalid author
          exit 1
        fi

        SSH_OPTS="-o UserKnownHostsFile=/tmp/.ssh/known_hosts \
                    -o GSSAPIAuthentication=yes \
                    -o GSSAPIDelegateCredentials=yes \
                    -o IdentitiesOnly=yes"

        # Read checksum configuration from secret
        checksum_user=$(cat /mnt/checksum_credentials/user)
        checksum_host=$(cat /mnt/checksum_credentials/host)

        sign_file() {
            local component_name=$1
            local sign_method=$2  # The signing method: --clearsign or --gpgsign
            local extension=$3
            pipeline_run_uid=$(context.taskRun.uid)
            output_path="/home/$checksum_user/${pipeline_run_uid}_${component_name}/checksum/sha256sum.txt.$extension"
            input_file="/home/$checksum_user/${pipeline_run_uid}_${component_name}/checksum/sha256sum.txt"

            echo "Executing SSH command with sign method: $sign_method for component: $component_name"
            # shellcheck disable=SC2029,SC2086
            ssh $SSH_OPTS "$checksum_user@$checksum_host" \
            "rpm-sign --nat $sign_method --key $SIGNING_KEY_NAME --onbehalfof=$AUTHOR \
            --output $output_path $input_file"
        }

        # Generate a kerberos ticket to ssh to the checksum host.
        # The ticket is required for interacting with rpm-sign as well,
        # so we use GSSAPI Delegate (in ssh opts) to transfer the ticket to the checksum host
        KRB5CCNAME=FILE:/tmp/krb5cc_$(id -u)
        export KRB5CCNAME
        base64 -d /mnt/checksum_credentials/keytab > /tmp/sa.keytab
        retry 5 kinit -kt /tmp/sa.keytab "${checksum_user}@$(params.kerberosRealm)"

        mkdir -p /tmp/.ssh
        chmod 700 /tmp/.ssh
        cp "/mnt/checksum_credentials/fingerprint" /tmp/.ssh/known_hosts
        chmod 600 /tmp/.ssh/known_hosts

        CONTENT_DIR=/shared/artifacts

        set +x
        oras login quay.io -u "$QUAY_USER" -p "$QUAY_PASS"
        set -x

        # Process each component separately
        NUM_COMPONENTS=$(jq '.components | length' <<< "$SNAPSHOT_JSON")
        for ((c = 0; c < NUM_COMPONENTS; c++)); do
          COMPONENT=$(jq -c --arg i "$c" '.components[$i|tonumber]' <<< "$SNAPSHOT_JSON")
          COMPONENT_NAME=$(jq -r '.name' <<< "$COMPONENT")
          COMPONENT_DIR="$CONTENT_DIR/$COMPONENT_NAME"

          echo "Generating checksums for component: $COMPONENT_NAME"

          # Create signed directory structure for this component
          SIGNED_DIR="$COMPONENT_DIR/signed"
          mkdir -p "$SIGNED_DIR"

          cd "$SIGNED_DIR"

          # Pull signed Mac binaries for this component (if mac content exists)
          # oras will create macos/ directory with amd64/ and arm64/ subdirectories
          if [ -f "$COMPONENT_DIR/has_mac" ]; then
            signed_mac_digest=$(cat "$COMPONENT_DIR/signed_mac_digest.txt")
            # shellcheck disable=SC2086
            # Pull directly to current directory ($SIGNED_DIR) - oras will create macos/ directory
            oras pull "$(params.quayURL)/${COMPONENT_NAME}/signed@${signed_mac_digest}"
          fi

          # Pull signed Windows binaries for this component (if windows content exists)
          # oras will create windows/ directory with amd64/ and arm64/ subdirectories
          if [ -f "$COMPONENT_DIR/has_windows" ]; then
            signed_windows_digest=$(cat "$COMPONENT_DIR/signed_windows_digest.txt")
            signed_windows_digest=${signed_windows_digest//[[:space:]]/}
            # shellcheck disable=SC2086
            # Pull directly to current directory ($SIGNED_DIR) - oras will create windows/ directory
            oras pull "$(params.quayURL)/${COMPONENT_NAME}/signed@${signed_windows_digest}"
          fi

          # Generate checksums for all binaries
          # Linux files remain in $COMPONENT_DIR/linux/ (not signed), while Mac/Windows are in $SIGNED_DIR/
          # The structure is os/arch/ so we recursively find all files
          SHA_SUM_PATH="$SIGNED_DIR/sha256sum.txt"
          touch "$SHA_SUM_PATH"
          
          # Generate checksums for signed binaries (macos, windows) from SIGNED_DIR
          CHECKSUM_DIRS=""
          [ -d "macos" ] && CHECKSUM_DIRS="$CHECKSUM_DIRS macos"
          [ -d "windows" ] && CHECKSUM_DIRS="$CHECKSUM_DIRS windows"
          
          if [ -n "$CHECKSUM_DIRS" ]; then
            # shellcheck disable=SC2086
            # Recursively find all files in os/arch/ structure
            find $CHECKSUM_DIRS -type f 2>/dev/null | while read -r file; do
                checksum=$(sha256sum "$file" | awk '{ print $1 }')
                echo "$checksum  $file" >> "$SHA_SUM_PATH"
            done
          fi
          
          # Generate checksums for Linux files from COMPONENT_DIR/linux/ (not signed)
          if [ -f "$COMPONENT_DIR/has_linux" ] && [ -d "$COMPONENT_DIR/linux" ]; then
            # shellcheck disable=SC2086
            # Recursively find all files in linux/os/arch/ structure
            find "$COMPONENT_DIR/linux" -type f 2>/dev/null | while read -r file; do
                checksum=$(sha256sum "$file" | awk '{ print $1 }')
                # Use relative path from COMPONENT_DIR for consistency
                prefix_len=$((${#COMPONENT_DIR} + 1))
                rel_path="${file:$prefix_len}"
                echo "$checksum  $rel_path" >> "$SHA_SUM_PATH"
            done
          fi

          # Send sha256sum.txt to the checksum host for signing (component-specific path)
          REMOTE_DIR="$(context.taskRun.uid)_${COMPONENT_NAME}"
          # shellcheck disable=SC2029,SC2086
          ssh $SSH_OPTS "${checksum_user}@${checksum_host}" "mkdir -p ~/${REMOTE_DIR}/checksum"
          # shellcheck disable=SC2086
          scp $SSH_OPTS "${SHA_SUM_PATH}" \
          "${checksum_user}@${checksum_host}:~/${REMOTE_DIR}/checksum"

          sign_file "$COMPONENT_NAME" --clearsign sig
          sign_file "$COMPONENT_NAME" --gpgsign gpg

          # Copy the signed checksum files back
          scp "$SSH_OPTS" \
          "${checksum_user}@${checksum_host}:~/${REMOTE_DIR}/checksum/sha256sum.txt.sig" \
          "${SIGNED_DIR}/sha256sum.txt.sig"

          scp "$SSH_OPTS" \
          "${checksum_user}@${checksum_host}:~/${REMOTE_DIR}/checksum/sha256sum.txt.gpg" \
          "${SIGNED_DIR}/sha256sum.txt.gpg"

          # Clean up remote checksum directory
          # shellcheck disable=SC2029,SC2086
          ssh $SSH_OPTS "${checksum_user}@${checksum_host}" "rm -rf ~/${REMOTE_DIR}"

          cd "$CONTENT_DIR"
        done

    - name: compress-artifacts
      image: quay.io/konflux-ci/release-service-utils@sha256:454e457cdadd5892b31fe2b7d4911b3d3a761f5705bb0cd30f864be0a597594b
      computeResources:
        limits:
          memory: 256Mi
        requests:
          memory: 256Mi
          cpu: 150m
      volumeMounts:
        - name: shared-dir
          mountPath: /shared
      env:
        - name: "SNAPSHOT_JSON"
          value: "$(params.snapshot_json)"
      script: |
        #!/usr/bin/env bash
        set -eux

        STDERR_FILE=/tmp/stderr.txt

        # Check if the previous step finished successfully. If not, stop here.
        if [ "$(cat "$(results.result.path)")" != "Success" ]; then
          echo "Previous step failed. Exiting..."
          exit 0
        fi

        exitfunc() {
            local err=$1
            local line=$2
            local command="$3"
            if [ "$err" -eq 0 ] ; then
                echo -n "Success" > "$(results.result.path)"
            else
                echo "$0: ERROR '$command' failed at line $line - exited with status $err" > "$(results.result.path)"
                if [ -f "$STDERR_FILE" ] ; then
                    # save output to the task result excluding the trace log
                    grep -v "^\+" "$STDERR_FILE" | tail -c 512 | tee -a "$(results.result.path)"
                fi
            fi
            exit 0 # exit the script cleanly as there is no point in proceeding past an error or exit call
        }
        # due to set -e, this catches all EXIT and ERR calls and the task should never fail with nonzero exit code
        trap 'exitfunc $? $LINENO "$BASH_COMMAND"' EXIT

        CONTENT_DIR=/shared/artifacts

        function compress_component {
          COMPONENT_JSON="$1"
          COMPONENT_NAME=$(jq -r '.name' <<< "$COMPONENT_JSON")
          
          # Per-component directories
          COMPONENT_DIR="$CONTENT_DIR/$COMPONENT_NAME"
          SIGNED_DIR="$COMPONENT_DIR/signed"
          READY_FOR_DIST_DIR="$COMPONENT_DIR/ready_for_distribution"
          
          mkdir -p "$READY_FOR_DIST_DIR"
          echo "Compressing artifacts for component: $COMPONENT_NAME"

          # Helper function to process a single file entry
          # Args: $1 = FILE JSON, $2 = "files" or "staged" (for logging)
          process_file_entry() {
            local FILE="$1"
            local ARRAY_NAME="$2"
            local SOURCE
            if ! SOURCE=$(jq -er '.source' <<< "$FILE" 2>/dev/null); then
              echo "Error: Missing ${ARRAY_NAME}[].source for component." >&2
              return 1
            fi
            local SOURCE_FILENAME
            SOURCE_FILENAME=$(basename "$SOURCE")
            local OS
            OS=$(jq -er '.os' <<< "$FILE")
            local ARCH
            ARCH=$(jq -er '.arch' <<< "$FILE")
            local DELIVERY_FORMAT
            DELIVERY_FORMAT=$(jq -r '.delivery_format // ["compressed"] | sort | .[]' <<< "$FILE")

            # Determine the binary file path and OS directory name based on OS and arch
            # New structure: os/arch/{binary_file}
            # Linux files remain in COMPONENT_DIR/linux/ (not signed), while Mac/Windows are in SIGNED_DIR/
            local OS_DIR=""
            local ARCH_DIR=""
            case "$OS" in
              (darwin)
                OS_DIR="macos"
                ARCH_DIR="${SIGNED_DIR}/${OS_DIR}/${ARCH}"
                ;;
              (linux)
                OS_DIR="linux"
                ARCH_DIR="${COMPONENT_DIR}/${OS_DIR}/${ARCH}"
                ;;
              (windows)
                OS_DIR="windows"
                ARCH_DIR="${SIGNED_DIR}/${OS_DIR}/${ARCH}"
                ;;
            esac

            if [ -z "$ARCH_DIR" ] || [ ! -d "$ARCH_DIR" ]; then
              echo "  Warning: Architecture directory not found for ${ARRAY_NAME} entry (os: ${OS}, arch: ${ARCH})" >&2
              return 1
            fi

            # Check that the arch directory exists and has content
            if [ ! -d "$ARCH_DIR" ] || [ -z "$(ls -A "$ARCH_DIR" 2>/dev/null)" ]; then
              echo "  Warning: Architecture directory is empty or not found: ${ARCH_DIR}" >&2
              return 1
            fi

            for FORMAT in $DELIVERY_FORMAT; do
              if [ "$FORMAT" = "compressed" ]; then
                case "$OS" in
                  (darwin|linux)
                    # Archive everything in os/arch/, preserving nested directory structure
                    # Archive from within os/arch/ so the final archive doesn't include os/arch/ prefix
                    tar -czf "${READY_FOR_DIST_DIR}/${SOURCE_FILENAME}" -C "${ARCH_DIR}" .
                    echo "  Created (${ARRAY_NAME}): ${SOURCE_FILENAME}"
                    ;;
                  (windows)
                    # Replace .tar.gz extension with .zip for Windows archives
                    local WINDOWS_FILENAME="${SOURCE_FILENAME}"
                    WINDOWS_FILENAME="${WINDOWS_FILENAME%.tar.gz}.zip"
                    # Create zip preserving nested directory structure from os/arch/
                    (cd "${ARCH_DIR}" && zip -r "${READY_FOR_DIST_DIR}/${WINDOWS_FILENAME}" .)
                    echo "  Created (${ARRAY_NAME}): ${WINDOWS_FILENAME}"
                    ;;
                esac
              fi

              if [ "$FORMAT" = "raw" ]; then
                # Copy all files from os/arch/, preserving nested structure
                cp -r "${ARCH_DIR}"/* "${READY_FOR_DIST_DIR}/" 2>/dev/null || true
                echo "  Copied (${ARRAY_NAME}): contents from ${ARCH_DIR}"
              fi
            done
          }

          # Process files[] array (Developer Portal files)
          NUM_MAPPED_FILES=$(jq '.files | length // 0' <<< "${COMPONENT_JSON}")
          UPDATED_FILES="[]"
          
          if [ "$NUM_MAPPED_FILES" -gt 0 ]; then
            echo "  Processing ${NUM_MAPPED_FILES} files from files[] (Developer Portal):"
            for ((i = 0; i < NUM_MAPPED_FILES; i++)) ; do
              FILE=$(jq -c --arg i "$i" '.files[$i|tonumber]' <<< "$COMPONENT_JSON")
              process_file_entry "$FILE" "files"
              
              # Track files for SNAPSHOT_JSON update
              SOURCE=$(jq -er '.source' <<< "$FILE" 2>/dev/null) || continue
              SOURCE_FILENAME=$(basename "$SOURCE")
              OS=$(jq -r '.os // empty' <<< "$FILE")
              if [ "$OS" = "windows" ]; then
                WINDOWS_SOURCE="${SOURCE%.tar.gz}.zip"
                UPDATED_FILES=$(jq --arg src "$WINDOWS_SOURCE" --argjson file "$FILE" \
                  '. + [($file | .source = $src)]' <<< "$UPDATED_FILES")
              else
                UPDATED_FILES=$(jq --argjson file "$FILE" '. + [$file]' <<< "$UPDATED_FILES")
              fi
            done
          fi

          # Process staged.files[] array (Customer Portal files)
          # These may be different files than files[], need separate processing
          NUM_STAGED_FILES=$(jq '.staged.files | length // 0' <<< "${COMPONENT_JSON}")
          
          if [ "$NUM_STAGED_FILES" -gt 0 ]; then
            echo "  Processing ${NUM_STAGED_FILES} files from staged.files[] (Customer Portal):"
            for ((i = 0; i < NUM_STAGED_FILES; i++)) ; do
              FILE=$(jq -c --arg i "$i" '.staged.files[$i|tonumber]' <<< "$COMPONENT_JSON")
              process_file_entry "$FILE" "staged.files"
            done
          fi

          # Copy checksum files to ready_for_distribution directory (for Developer Portal)
          cp "${SIGNED_DIR}/sha256sum.txt" "${READY_FOR_DIST_DIR}/" 2>/dev/null || true
          cp "${SIGNED_DIR}/sha256sum.txt.sig" "${READY_FOR_DIST_DIR}/" 2>/dev/null || true
          cp "${SIGNED_DIR}/sha256sum.txt.gpg" "${READY_FOR_DIST_DIR}/" 2>/dev/null || true

          # Update the component in SNAPSHOT_JSON with the corrected filenames (for files[] array)
          SNAPSHOT_JSON=$(jq --arg name "$COMPONENT_NAME" --argjson updated_files "$UPDATED_FILES" '
            .components |= map(
              if .name == $name then
                .files = $updated_files
              else
                .
              end
            )' <<< "$SNAPSHOT_JSON")
        }

        NUM_COMPONENTS=$(jq '.components | length' <<< "$SNAPSHOT_JSON")
        for ((c = 0; c < NUM_COMPONENTS; c++)) ; do
            COMPONENT=$(jq -c --arg i "$c" '.components[$i|tonumber]' <<< "$SNAPSHOT_JSON")
            compress_component "$COMPONENT" 2> "$STDERR_FILE"
        done

        # Save the modified snapshot to shared volume so subsequent steps can use it
        echo "$SNAPSHOT_JSON" > /shared/snapshot.json

    - name: push-artifacts
      image: quay.io/konflux-ci/release-service-utils@sha256:454e457cdadd5892b31fe2b7d4911b3d3a761f5705bb0cd30f864be0a597594b
      computeResources:
        limits:
          memory: 512Mi
        requests:
          memory: 512Mi
          cpu: 250m
      volumeMounts:
        - name: shared-dir
          mountPath: /shared
        - name: exodus-gw-secret
          mountPath: /mnt/exodusGwSecret
        - name: pulp-secret
          mountPath: /mnt/pulpSecret
        - name: udcache-secret
          mountPath: /mnt/udcacheSecret
        - name: cgw-secret
          mountPath: /mnt/cgwSecret
      env:
        - name: "SNAPSHOT_JSON"
          value: "$(params.snapshot_json)"
        # Force Python requests to use system CA bundle (which includes RH internal CAs)
        # instead of certifi's bundled CAs. This was causing SSL verification failures with pulp
        - name: "REQUESTS_CA_BUNDLE"
          value: "/etc/pki/tls/certs/ca-bundle.crt"
      script: |
        #!/usr/bin/env bash
        set -e

        # Check certificate expiration for all certificates used in this step
        echo "=== Checking certificate expiration ==="

        # Check Exodus Gateway certificate
        echo "Checking Exodus Gateway certificate"
        if ! check_cert_expiration "/mnt/exodusGwSecret/cert" "$(params.certExpirationWarnDays)"; then
          echo "ERROR: Exodus Gateway certificate validation failed"
          exit 1
        fi

        # Check Pulp certificate
        echo "Checking Pulp certificate"
        if ! check_cert_expiration "/mnt/pulpSecret/konflux-release-rhsm-pulp.crt" \
            "$(params.certExpirationWarnDays)"; then
          echo "ERROR: Pulp certificate validation failed"
          exit 1
        fi

        # Check UDCache certificate
        echo "Checking UDCache certificate"
        if ! check_cert_expiration "/mnt/udcacheSecret/cert" "$(params.certExpirationWarnDays)"; then
          echo "ERROR: UDCache certificate validation failed"
          exit 1
        fi

        echo "=== All certificates are valid ==="

        # Use the modified snapshot from shared volume if available
        if [ -f /shared/snapshot.json ]; then
          SNAPSHOT_JSON="$(cat /shared/snapshot.json)"
        fi

        EXODUS_CERT="$(cat /mnt/exodusGwSecret/cert)"
        EXODUS_KEY="$(cat /mnt/exodusGwSecret/key)"
        EXODUS_URL="$(cat /mnt/exodusGwSecret/url)"
        PULP_URL="$(cat /mnt/pulpSecret/pulp_url)"
        PULP_CERT="$(cat /mnt/pulpSecret/konflux-release-rhsm-pulp.crt)"
        PULP_KEY="$(cat /mnt/pulpSecret/konflux-release-rhsm-pulp.key)"
        UDC_URL="$(cat /mnt/udcacheSecret/url)"
        UDC_CERT="$(cat /mnt/udcacheSecret/cert)"
        UDC_KEY="$(cat /mnt/udcacheSecret/key)"
        CGW_USERNAME="$(cat /mnt/cgwSecret/username)"
        CGW_PASSWORD="$(cat /mnt/cgwSecret/token)"

        # export the variables used by the scripts in release-service-utils
        export CGW_USERNAME CGW_PASSWORD

        set -x

        STDERR_FILE=/tmp/stderr.txt

        # Check if the previous step finished successfully. If not, stop here.
        if [ "$(cat "$(results.result.path)")" != "Success" ]; then
          echo "Previous step failed. Exiting..."
          exit 0
        fi

        exitfunc() {
            local err=$1
            local line=$2
            local command="$3"
            if [ "$err" -eq 0 ] ; then
                echo -n "Success" > "$(results.result.path)"
            else
                # cleaning up the publishedFiles result to clean up space in the results buffer
                echo > "$(results.publishedFiles.path)"
                echo "$0: ERROR '$command' failed at line $line - exited with status $err" > "$(results.result.path)"
                if [ -f "$STDERR_FILE" ] ; then
                    # save output to the task result excluding the trace log
                    grep -v "^\+" "$STDERR_FILE" | tail -c 512 | tee -a "$(results.result.path)"
                fi
            fi
            exit 0 # exit the script cleanly as there is no point in proceeding past an error or exit call
        }
        # due to set -e, this catches all EXIT and ERR calls and the task should never fail with nonzero exit code
        trap 'exitfunc $? $LINENO "$BASH_COMMAND"' EXIT

        # Setup required variables
        export EXODUS_GW_CERT=/tmp/exodus.crt
        export EXODUS_GW_KEY=/tmp/exodus.key
        EXODUS_CONF_PATH=/tmp/exodus-rsync.conf
        export PULP_CERT_FILE=/tmp/pulp.crt
        export PULP_KEY_FILE=/tmp/pulp.key
        export UDCACHE_CERT=/tmp/udc.crt
        export UDCACHE_KEY=/tmp/udc.key
        EXODUS_GW_ENV=$(params.exodusGwEnv)
        export EXODUS_GW_ENV
        export EXODUS_GW_URL="$EXODUS_URL"
        export EXODUS_PULP_HOOK_ENABLED=True
        export EXODUS_GW_TIMEOUT=7200

        set +x
        echo "$EXODUS_CERT" > "$EXODUS_GW_CERT"
        echo "$EXODUS_KEY" > "$EXODUS_GW_KEY"
        echo "$PULP_CERT" > "$PULP_CERT_FILE"
        echo "$PULP_KEY" > "$PULP_KEY_FILE"
        echo "$UDC_CERT" > "$UDCACHE_CERT"
        echo "$UDC_KEY" > "$UDCACHE_KEY"
        set -x

        CONTENT_DIR="/shared/artifacts"

        # save the results - list all files from all component ready_for_distribution directories
        find "${CONTENT_DIR}"/*/ready_for_distribution -type f -exec basename {} \; 2>/dev/null \
          | tail -c 512 | tee "$(results.publishedFiles.path)"

        create_exodus_conf() {
          echo "Creating Exodus configuration file....."
          set +x
          cat >"$EXODUS_CONF_PATH" <<EOF
        gwcert:   $EXODUS_GW_CERT
        gwkey:    $EXODUS_GW_KEY
        gwurl:    $EXODUS_GW_URL
        gwenv:    $EXODUS_GW_ENV

        logger: file:/proc/1/fd/1
        loglevel: info

        environments:
          - prefix: exodus
        EOF
          set -x
        }

        push_component_to_pulp() {
          local component_name=$1
          local component_dir="${CONTENT_DIR}/${component_name}/ready_for_distribution"
          
          # Get version and destination from the component's staged config
          version=$(jq -cr --arg name "$component_name" '
            .components[] | select(.name == $name) | .staged.version // ""
          ' <<< "$SNAPSHOT_JSON")
          
          staged_destination=$(jq -cr --arg name "$component_name" '
            .components[] | select(.name == $name) | .staged.destination // ""
          ' <<< "$SNAPSHOT_JSON")
          
          if [[ -z "$version" || -z "$staged_destination" ]]; then
            echo "Error: Pulp push requires both staged.version and staged.destination" \
              "for component $component_name" >&2
            exit 1
          fi

          echo "Pushing component $component_name to customer portal with pulp"
          
          # Create proper directory structure for Pulp with the destination (pulp repo) path
          # The relative_path in staged.yaml must include the pulp repo name for content
          # to be properly mapped and exposed via repo notes on the Customer Portal
          staging_dir=$(mktemp -d)
          mkdir -p "${staging_dir}/${staged_destination}/FILES"
          
          # Get the list of files to include from RPA - Customer Portal uses staged.files ONLY
          # Checksums go to Developer Portal automatically but NOT to Customer Portal unless listed
          COMPONENT_JSON=$(jq -c --arg name "$component_name" '
            .components[] | select(.name == $name)
          ' <<< "$SNAPSHOT_JSON")
          
          # Check if there are Customer Portal files to push
          NUM_STAGED_FILES=$(jq '.staged.files | length // 0' <<< "$COMPONENT_JSON")
          if [[ "$NUM_STAGED_FILES" -eq 0 ]]; then
            echo "Warning: No staged.files specified in RPA for Customer Portal push" >&2
            echo "  Customer Portal requires explicit staged.files array" >&2
            rm -rf "$staging_dir"
            return 0
          fi
          
          # Copy only the files listed in staged.files to the staging directory
          for ((i = 0; i < NUM_STAGED_FILES; i++)); do
            source_path=$(jq -r --arg i "$i" '.staged.files[$i|tonumber].source // empty' <<< "$COMPONENT_JSON")
            # Get destination filename from staged.files[].filename (for renaming)
            dest_filename=$(jq -r --arg i "$i" '.staged.files[$i|tonumber].filename // empty' <<< "$COMPONENT_JSON")
            if [[ -n "$source_path" ]]; then
              # Get the source filename from path (basename) - this is what the file is named locally
              source_filename=$(basename "$source_path")
              # Handle windows files that may have been converted from .tar.gz to .zip
              if [[ "$source_filename" == *"windows"* && "$source_filename" == *.tar.gz ]]; then
                zip_filename="${source_filename%.tar.gz}.zip"
                if [[ -f "$component_dir/$zip_filename" ]]; then
                  source_filename="$zip_filename"
                  # Also update dest_filename if it was .tar.gz
                  if [[ "$dest_filename" == *.tar.gz ]]; then
                    dest_filename="${dest_filename%.tar.gz}.zip"
                  fi
                fi
              fi
              # Use dest_filename if specified, otherwise fall back to source_filename
              if [[ -z "$dest_filename" ]]; then
                dest_filename="$source_filename"
              fi
              if [[ -f "$component_dir/$source_filename" ]]; then
                cp "$component_dir/$source_filename" "${staging_dir}/${staged_destination}/FILES/${dest_filename}"
                echo "  Including file for Customer Portal: $source_filename -> $dest_filename"
              else
                echo "  Warning: File not found: $source_filename" >&2
              fi
            fi
          done
          
          cd "$staging_dir"
          
          # Generate staged.yaml only for files that were copied
          staged_json='{"header":{"version": "0.2"},"payload":{"files":[]}}'
          # shellcheck disable=SC2035
          while IFS= read -r -d '' file ; do
              staged_json=$(jq --arg filename "$(basename "$file")" --arg path "$file" \
                --arg version "$version" \
                '.payload.files[.payload.files | length] =
                {"filename": $filename, "relative_path": $path, "version": $version}' <<< "$staged_json")
          done < <(find * -type f -print0)
          echo "$staged_json" | yq -P -I 4 > staged.yaml

          pulp_push_wrapper --source "$staging_dir" --pulp-url "$PULP_URL" \
            --pulp-cert $PULP_CERT_FILE --pulp-key $PULP_KEY_FILE --udcache-url "$UDC_URL" \
            2> "$STDERR_FILE"

          rm -rf "$staging_dir"
          cd "$CONTENT_DIR"
        }

        push_component_to_cdn() {
          local component_name=$1
          local component_dir="${CONTENT_DIR}/${component_name}/ready_for_distribution"
          
          echo "Pushing component $component_name to CDN with exodus-rsync"
          create_exodus_conf
          prefix="exodus:/content/origin/files/sha256"
          
          cd "$component_dir"
          # shellcheck disable=SC2035
          while IFS= read -r -d '' file; do
            set +x
            file_name=$(basename "$file")
            checksum=$(sha256sum "$file" | awk '{print $1}')
            destination_path="$prefix/${checksum:0:2}/$checksum/$file_name"
            set -x

            rsync --exodus-conf "$EXODUS_CONF_PATH" "$file" "$destination_path"
          done < <(find * -type f -print0)
          cd "$CONTENT_DIR"
        }

        publish_component_to_cgw() {
          local component_name=$1
          local component_dir="${CONTENT_DIR}/${component_name}/ready_for_distribution"
          
          # Update contentDir for this component in SNAPSHOT_JSON
          SNAPSHOT_JSON=$(
            jq --arg name "$component_name" --arg dir "$component_dir" '
              .components |= map(
                if .name == $name and .contentGateway? then
                  .contentGateway.contentDir = $dir
                else
                  .
                end
              )
            ' <<<"$SNAPSHOT_JSON"
          )
        }

        # Process each component
        NUM_COMPONENTS=$(jq '.components | length' <<< "$SNAPSHOT_JSON")
        for ((c = 0; c < NUM_COMPONENTS; c++)); do
          COMPONENT=$(jq -c --arg i "$c" '.components[$i|tonumber]' <<< "$SNAPSHOT_JSON")
          COMPONENT_NAME=$(jq -r '.name' <<< "$COMPONENT")
          
          HAS_STAGED=$(jq -r '.staged | length > 0' <<< "$COMPONENT")
          HAS_CGW=$(jq -r '.contentGateway | length > 0' <<< "$COMPONENT")

          echo "Processing component: $COMPONENT_NAME (staged=$HAS_STAGED, cgw=$HAS_CGW)"

          if [[ "$HAS_STAGED" == "true" ]]; then
            # Push to customer portal (pulp) - CDN push not needed when using pulp
            push_component_to_pulp "$COMPONENT_NAME" 2> >(tee "$STDERR_FILE" >&2)
          elif [[ "$HAS_CGW" == "true" ]]; then
            # Only push to CDN when NOT pushing to pulp
            push_component_to_cdn "$COMPONENT_NAME" > >(tee "$STDERR_FILE") 2>&1 || true
          fi
          
          if [[ "$HAS_CGW" == "true" ]]; then
            publish_component_to_cgw "$COMPONENT_NAME"
          fi
        done

        # Final CGW publish with all components' contentDir set
        CGW_PUSH=$(jq 'any(.components[]?.contentGateway?; length > 0)' <<< "$SNAPSHOT_JSON")
        if [[ "$CGW_PUSH" == "true" ]]; then
          echo "Publishing all components to CGW..."
          publish_to_cgw_wrapper \
            --cgw_host "$(params.cgwHostname)" \
            --data_json "$SNAPSHOT_JSON" 2> >(tee "$STDERR_FILE" >&2)
        fi
