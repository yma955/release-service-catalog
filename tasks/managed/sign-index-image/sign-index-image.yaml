---
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: sign-index-image
  annotations:
    tekton.dev/pipelines.minVersion: "0.12.1"
    tekton.dev/tags: release
spec:
  description: |-
    Creates an InternalRequest to sign an index image
    
    ## Signing data parameters
    
     The signing configuration should be set as `data.sign` in the _releasePlanAdmission_. Previously it used to be
     set also in the `data.fbc`. The data should be set in the _ReleasePlanAdmission_ as follows:
    
    ```
    data:
        sign:
            request: <signing pipeline name>
            pipelineImage: <image pullspec>
            configMapName: <configmap name>
    ```
  params:
    - name: dataPath
      description: Path to the JSON string of the merged data to use in the data workspace
      type: string
    - name: releasePlanAdmissionPath
      description: Path to the JSON string of the releasePlanAdmission in the data workspace
      type: string
    - name: requester
      type: string
      description: Name of the user that requested the signing, for auditing purposes
    - name: requestTimeout
      type: string
      default: "1800"
      description: InternalRequest timeout
    - name: pipelineRunUid
      type: string
      description: The uid of the current pipelineRun. Used as a label value when creating internal requests
    - name: fbcResultsPath
      type: string
      description: Path to the JSON file in the data workspace containing fbc results
    - name: concurrentLimit
      type: string
      description: The maximum number of concurrent signing requests
      default: 16
    - name: ociStorage
      description: The OCI repository where the Trusted Artifacts are stored
      type: string
      default: "empty"
    - name: ociArtifactExpiresAfter
      description: Expiration date for the trusted artifacts created in the
        OCI repository. An empty string means the artifacts do not expire
      type: string
      default: "1d"
    - name: trustedArtifactsDebug
      description: Flag to enable debug logging in trusted artifacts. Set to a non-empty string to enable
      type: string
      default: ""
    - name: orasOptions
      description: oras options to pass to Trusted Artifacts calls
      type: string
      default: ""
    - name: sourceDataArtifact
      type: string
      description: Location of trusted artifacts to be used to populate data directory
      default: ""
    - name: dataDir
      description: The location where data will be stored
      type: string
      default: /var/workdir/release
    - name: taskGitUrl
      type: string
      description: The url to the git repo where the release-service-catalog tasks and stepactions to be used are stored
    - name: taskGitRevision
      type: string
      description: The revision in the taskGitUrl repo to be used
    - name: pyxisServer
      type: string
      description: The server type to use. Options are 'production','production-internal,'stage-internal' and 'stage'
      default: production
    - name: pyxisSecret
      type: string
      description: |
        The kubernetes secret to use to authenticate to Pyxis. It needs to contain two keys: key and cert
    - name: batchLimit
      type: string
      description: |
        size of batch attributes to send to internal-request. As internal request arguments are need to be
        strings, size here represent maximal string length of `references` and `manifest_digests` sent to
        internal request
      default: 4096
    - name: caTrustConfigMapName
      type: string
      description: The name of the ConfigMap to read CA bundle data from
      default: trusted-ca
    - name: caTrustConfigMapKey
      type: string
      description: The name of the key in the ConfigMap that contains the CA bundle data
      default: ca-bundle.crt
  results:
    - name: sourceDataArtifact
      type: string
      description: Produced trusted data artifact
  volumes:
    - name: workdir
      emptyDir: {}
    - name: pyxis-secret-vol
      secret:
        secretName: $(params.pyxisSecret)
        defaultMode: 0444
    - name: trusted-ca
      configMap:
        name: $(params.caTrustConfigMapName)
        items:
          - key: $(params.caTrustConfigMapKey)
            path: ca-bundle.crt
        optional: true

  stepTemplate:
    securityContext:
      runAsUser: 1001 
    volumeMounts:
      - mountPath: /var/workdir
        name: workdir
      - name: trusted-ca
        mountPath: /mnt/trusted-ca
        readOnly: true
    env:
      - name: IMAGE_EXPIRES_AFTER
        value: $(params.ociArtifactExpiresAfter)
      - name: "ORAS_OPTIONS"
        value: "$(params.orasOptions)"
      - name: "DEBUG"
        value: "$(params.trustedArtifactsDebug)"
  steps:
    - name: use-trusted-artifact
      computeResources:
        limits:
          memory: 64Mi
        requests:
          memory: 64Mi
          cpu: 30m
      ref:
        resolver: "git"
        params:
          - name: url
            value: $(params.taskGitUrl)
          - name: revision
            value: $(params.taskGitRevision)
          - name: pathInRepo
            value: stepactions/use-trusted-artifact/use-trusted-artifact.yaml
      params:
        - name: workDir
          value: $(params.dataDir)
        - name: sourceDataArtifact
          value: $(params.sourceDataArtifact)
    - name: sign-index-image
      image: quay.io/konflux-ci/release-service-utils:02a8ddcd16113371a255fd1ef0a196399d300162
      computeResources:
        limits:
          memory: 1Gi
        requests:
          memory: 1Gi
          cpu: 250m
      volumeMounts:
        - name: pyxis-secret-vol
          mountPath: "/etc/secrets"
      env:
        - name: BATCH_LIMIT
          value: $(params.batchLimit)
      script: |
        #!/usr/bin/env bash
        set -e

        pyxisCert="$(cat /etc/secrets/cert)"
        pyxisKey="$(cat /etc/secrets/key)"

        set -x

        RUNNING_JOBS="\j" # Bash parameter for number of jobs currently running
        CONCURRENT_LIMIT=$(params.concurrentLimit)
        REQUEST_COUNT=0

        TASK_LABEL="internal-services.appstudio.openshift.io/group-id"
        TASK_ID=$(context.taskRun.uid)
        PIPELINERUN_LABEL="internal-services.appstudio.openshift.io/pipelinerun-uid"

        DATA_FILE="$(params.dataDir)/$(params.dataPath)"
        if [ ! -f "${DATA_FILE}" ] ; then
            echo "No valid data file was provided."
            exit 1
        fi

        RESULTS_FILE="$(params.dataDir)/$(params.fbcResultsPath)"
        if [ ! -f "${RESULTS_FILE}" ] ; then
            echo "No valid results file was provided."
            exit 1
        fi

        set +x
        echo "${pyxisCert:?}" > /tmp/crt
        echo "${pyxisKey:?}" > /tmp/key
        set -x

        export PYXIS_CERT_PATH=/tmp/crt
        export PYXIS_KEY_PATH=/tmp/key

        requestType=$(jq -r '.requestType // "internal-request"' "${DATA_FILE}")
        if [ "${requestType}" == "internal-pipelinerun" ] ; then
          RPA_FILE="$(params.dataDir)/$(params.releasePlanAdmissionPath)"
          if [ ! -f "${RPA_FILE}" ] ; then
              echo "No valid rpa file was provided."
              exit 1
          fi
          service_account_name=$(jq -r '.spec.pipeline.serviceAccountName // "release-service-account"' "${RPA_FILE}")
          EXTRA_ARGS=(
          --service-account "${service_account_name}"
          )
        else
          requestType=internal-request
          EXTRA_ARGS=()
        fi
        request=$(jq -r '.sign.request // "simple-signing-pipeline"' "${DATA_FILE}")

        default_pipeline_image="quay.io/redhat-isv/operator-pipelines-images:released"
        pipeline_image=$(jq -r --arg default_pipeline_image "${default_pipeline_image}" \
            '.sign.pipelineImage // .fbc.pipelineImage // $default_pipeline_image' "${DATA_FILE}")
        config_map_name=$(jq -r '.sign.configMapName // .fbc.configMapName // "signing-config-map"' "${DATA_FILE}")

        declare -a to_sign_references=()
        declare -a to_sign_digests=()
        declare -a to_sign_repos=()

        component_count=$(jq '.components | length' "$RESULTS_FILE")
        for ((i = 0; i < component_count; i++)); do
          reference_image=$(jq -r ".components[$i].target_index" "$RESULTS_FILE")

          # Translate direct quay.io reference to public facing registry reference
          # quay.io/redhat/product----repo -> registry.redhat.io/product/repo
          reference_image=$(translate-delivery-repo "$reference_image" \
            | jq -r '.[] | select(.repo=="redhat.io") | .url')

          digest_count=$(jq ".components[$i].image_digests | length" "$RESULTS_FILE")
          for ((j = 0; j < digest_count; j++)); do
            manifest_digest=$(jq -r ".components[$i].image_digests[$j]" "$RESULTS_FILE")

            rh_registry_repo=$(jq -r ".components[$i][\"rh-registry-repo\"]" "$RESULTS_FILE")
            repository="${rh_registry_repo#*/}"

            to_sign_references+=("${reference_image}")
            to_sign_digests+=("${manifest_digest}")
            to_sign_repos+=("${repository}")

          done
        done

        references_batch=""
        digests_batch=""
        repos_batch=""

        # Process to sign arrays in batches
        for i in "${!to_sign_references[@]}"; do
          new_references_batch="${references_batch}${to_sign_references[$i]} "
          new_digests_batch="${digests_batch}${to_sign_digests[$i]} "
          new_repos_batch="${repos_batch}${to_sign_repos[$i]} "

          # if batches are too big, send the request
          if [[ ${#new_references_batch} -gt $BATCH_LIMIT || ${#new_digests_batch} -gt $BATCH_LIMIT ]]; then

            while (( ${RUNNING_JOBS@P} >= "$CONCURRENT_LIMIT" )); do
              wait -n
            done

            ${requestType} --pipeline "${request}" \
              -p pipeline_image="${pipeline_image}" \
              -p references="${references_batch}" \
              -p manifest_digests="${digests_batch}" \
              -p repositories="${repos_batch}" \
              -p requester="$(params.requester)" \
              -p config_map_name="${config_map_name}" \
              -p taskGitUrl="$(params.taskGitUrl)" \
              -p taskGitRevision="$(params.taskGitRevision)" \
              -l ${TASK_LABEL}="${TASK_ID}" \
              -l ${PIPELINERUN_LABEL}="$(params.pipelineRunUid)" \
              -t "$(params.requestTimeout)" --pipeline-timeout "0h30m0s" --task-timeout "0h25m0s" \
              "${EXTRA_ARGS[@]}" -s true &
              ((++REQUEST_COUNT))
              echo "Request Count: $REQUEST_COUNT"

            # next batches consist of values which didn't fit in the previous batches
            references_batch="${to_sign_references[$i]} "
            digests_batch="${to_sign_digests[$i]} "
            repos_batch="${to_sign_repos[$i]} "
          else
            # if batches are still small enough, add the values to the batches
            references_batch="${new_references_batch}"
            digests_batch="${new_digests_batch}"
            repos_batch="${new_repos_batch}"
          fi
        done

        # Process the last batch
        if [[ ${#references_batch} -gt 0 ]]; then
          while (( ${RUNNING_JOBS@P} >= "$CONCURRENT_LIMIT" )); do
            wait -n
          done

          ${requestType} \
            --pipeline "${request}" \
            -p pipeline_image="${pipeline_image}" \
            -p references="${references_batch[*]}" \
            -p manifest_digests="${digests_batch[*]}" \
            -p repositories="${repos_batch[*]}" \
            -p config_map_name="${config_map_name}" \
            -p requester="$(params.requester)" \
            -p taskGitUrl="$(params.taskGitUrl)" \
            -p taskGitRevision="$(params.taskGitRevision)" \
            -l ${TASK_LABEL}="${TASK_ID}" \
            -l ${PIPELINERUN_LABEL}="$(params.pipelineRunUid)" \
            -t "$(params.requestTimeout)" --pipeline-timeout "0h30m0s" --task-timeout "0h25m0s" \
            "${EXTRA_ARGS[@]}" -s true &
        fi

        echo "Waiting for remaining processes to finish..."
        while (( ${RUNNING_JOBS@P} > 0 )); do
          wait -n
        done

        echo "done"
    - name: create-trusted-artifact
      computeResources:
        limits:
          memory: 128Mi
        requests:
          memory: 128Mi
          cpu: 250m
      ref:
        resolver: "git"
        params:
          - name: url
            value: $(params.taskGitUrl)
          - name: revision
            value: $(params.taskGitRevision)
          - name: pathInRepo
            value: stepactions/create-trusted-artifact/create-trusted-artifact.yaml
      params:
        - name: ociStorage
          value: $(params.ociStorage)
        - name: workDir
          value: $(params.dataDir)
        - name: sourceDataArtifact
          value: $(results.sourceDataArtifact.path)
