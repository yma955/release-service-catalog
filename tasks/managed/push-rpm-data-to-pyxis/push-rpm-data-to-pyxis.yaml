---
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: push-rpm-data-to-pyxis
  annotations:
    tekton.dev/pipelines.minVersion: "0.12.1"
    tekton.dev/tags: release
spec:
  description: |-
    Tekton task that extracts all rpms from the sboms and pushes them to Pyxis as an RPM Manifest.
    In addition, it will also update ContainerImage.content_sets field in Pyxis to include
    all repository_id strings found in rpm purl strings in the sboms.
  params:
    - name: pyxisJsonPath
      description: Path to the JSON string of the saved Pyxis data in the data workspace
      type: string
    - name: pyxisSecret
      type: string
      description: |
        The kubernetes secret to use to authenticate to Pyxis. It needs to contain two keys: key and cert
    - name: server
      type: string
      description: The server type to use. Options are 'production','production-internal,'stage-internal' and 'stage'
      default: production
    - name: concurrentLimit
      type: string
      description: The maximum number of images to be processed at once
      default: 20
    - name: ociStorage
      description: The OCI repository where the Trusted Artifacts are stored
      type: string
      default: "empty"
    - name: ociArtifactExpiresAfter
      description: Expiration date for the trusted artifacts created in the
        OCI repository. An empty string means the artifacts do not expire
      type: string
      default: "1d"
    - name: trustedArtifactsDebug
      description: Flag to enable debug logging in trusted artifacts. Set to a non-empty string to enable
      type: string
      default: ""
    - name: orasOptions
      description: oras options to pass to Trusted Artifacts calls
      type: string
      default: ""
    - name: sourceDataArtifact
      type: string
      description: Location of trusted artifacts to be used to populate data directory
      default: ""
    - name: dataDir
      description: The location where data will be stored
      type: string
      default: /var/workdir/release
    - name: taskGitUrl
      type: string
      description: The url to the git repo where the release-service-catalog tasks and stepactions to be used are stored
    - name: taskGitRevision
      type: string
      description: The revision in the taskGitUrl repo to be used
  results:
    - description: Produced trusted data artifact
      name: sourceDataArtifact
      type: string
  volumes:
    - name: workdir
      emptyDir: {}
    - name: pyxis-secret-vol
      secret:
        secretName: $(params.pyxisSecret)
        defaultMode: 0400
  stepTemplate:
    volumeMounts:
      - mountPath: /var/workdir
        name: workdir
    env:
      - name: IMAGE_EXPIRES_AFTER
        value: $(params.ociArtifactExpiresAfter)
      - name: "ORAS_OPTIONS"
        value: "$(params.orasOptions)"
      - name: "DEBUG"
        value: "$(params.trustedArtifactsDebug)"
      - name: RETRIES
        value: "3"  # Default number of retries for cosign download
  steps:
    - name: use-trusted-artifact
      computeResources:
        limits:
          memory: 64Mi
        requests:
          memory: 64Mi
          cpu: 30m
      ref:
        resolver: "git"
        params:
          - name: url
            value: $(params.taskGitUrl)
          - name: revision
            value: $(params.taskGitRevision)
          - name: pathInRepo
            value: stepactions/use-trusted-artifact/use-trusted-artifact.yaml
      params:
        - name: workDir
          value: $(params.dataDir)
        - name: sourceDataArtifact
          value: $(params.sourceDataArtifact)
    - name: download-sbom-files
      image:
        quay.io/konflux-ci/release-service-utils:fe734aa04168690e96f0a729f93845e7c70b7934
      computeResources:
        limits:
          memory: 512Mi
        requests:
          memory: 512Mi
          cpu: 500m
      script: |
        #!/usr/bin/env bash
        set -eux

        PYXIS_FILE="$(params.dataDir)/$(params.pyxisJsonPath)"
        if [ ! -f "${PYXIS_FILE}" ] ; then
            echo "No valid pyxis file was provided."
            exit 1
        fi

        # Create a mapping of unique imageIds to their SBOM files for efficient download
        echo "Creating unique imageId mapping for efficient SBOM download..."
        UNIQUE_IMAGES_FILE="$(mktemp)"
        jq '
        # Flatten all pyxisImages and preserve component containerImage
        [.components[] as $c | $c.pyxisImages[] | {
          imageId: .imageId,
          containerImage: $c.containerImage,
          pyxisImage: .
        }]
        | group_by(.imageId)
        | map({
          imageId: .[0].imageId,
          containerImage: .[0].containerImage,
          pyxisImage: .[0].pyxisImage,
          # Include all instances for later processing
          instances: .
        })' "${PYXIS_FILE}" > "${UNIQUE_IMAGES_FILE}"

        SBOM_PATH="/var/workdir/downloaded-sboms"
        # The dir might already exist in case of retries of the task.
        # No need for a cleanup - we will just override the files.
        mkdir -p "${SBOM_PATH}"
        cd "${SBOM_PATH}"

        # Function to run cosign with retries. It will pass all arguments to cosign.
        run_cosign () {
            attempt=0
            backoff1=2
            backoff2=3
            until [ "$attempt" -gt "${RETRIES}" ] ; do # 3 retries by default
                cosign "$@" && break
                sleep $backoff2

                # Fibbonaci backoff
                old_backoff1=$backoff1
                backoff1=$backoff2
                backoff2=$((old_backoff1 + backoff2))
                attempt=$((attempt+1))
            done
            if [ "$attempt" -gt "${RETRIES}" ] ; then
              echo "Max retries exceeded."
              exit 1
            fi
        }

        # Function to download SBOM for a single image
        download_sbom_for_image() {
            local pyxis_image="$1"
            local image_url="$2"

            local file
            local digest
            local arch_digest
            local os
            local arch
            local platform

            file="$(jq -r '.imageId' <<< "$pyxis_image").json"
            digest="$(jq -r '.digest' <<< "$pyxis_image")"
            arch_digest="$(jq -r '.arch_digest' <<< "$pyxis_image")"

            # cosign has very limited support for selecting the right auth entry,
            # so create a custom auth file with just one entry.
            DOCKER_CONFIG="$(mktemp -d)"
            export DOCKER_CONFIG
            select-oci-auth "${image_url}" > "${DOCKER_CONFIG}/config.json"

            # You can't pass --platform to a single arch image or cosign errors.
            # If digest equals arch_digest, then it's a single arch image
            if [ "$digest" = "$arch_digest" ] ; then
              echo "Fetching sbom for single arch image: $image_url to: $file"
              run_cosign download sbom --output-file "${file}" "${image_url}"
            else
              os=$(jq -r '.os' <<< "$pyxis_image")
              arch=$(jq -r '.arch' <<< "$pyxis_image")
              platform="${os}/${arch}"
              echo "Fetching sbom for image: $image_url with platform: $platform to: $file"
              run_cosign download sbom --output-file "${file}" --platform "${platform}" "${image_url}"
            fi
        }

        # Download SBOMs only for unique imageIds in parallel
        UNIQUE_IMAGES_COUNT=$(jq '. | length' "${UNIQUE_IMAGES_FILE}")
        echo "Downloading SBOMs for $UNIQUE_IMAGES_COUNT unique images in parallel..."

        # Collect all unique download tasks first
        declare -a download_tasks=()
        declare -a task_images=()
        declare -a task_urls=()

        for (( i=0; i < UNIQUE_IMAGES_COUNT; i++ )); do
          UNIQUE_IMAGE=$(jq -c --argjson i "$i" '.[$i]' "${UNIQUE_IMAGES_FILE}")
          IMAGEID=$(jq -r '.imageId' <<< "${UNIQUE_IMAGE}")
          CONTAINER_IMAGE=$(jq -r '.containerImage' <<< "${UNIQUE_IMAGE}")
          PYXIS_IMAGE=$(jq -c '.pyxisImage' <<< "${UNIQUE_IMAGE}")

          # Store task information for parallel execution
          download_tasks+=("$PYXIS_IMAGE")
          task_images+=("$IMAGEID")
          task_urls+=("$CONTAINER_IMAGE")
        done

        # Process downloads with continuous job management
        N=$(params.concurrentLimit)  # The maximum number of images to be processed at once
        RUNNING_JOBS="\j" # Bash parameter for number of jobs currently running
        success=true
        total=${#download_tasks[@]}
        echo "Starting SBOM download for $total unique images in total. " \
          "Up to $N images will be downloaded at once..."

        for (( idx=0; idx < total; idx++ )); do
          PYXIS_IMAGE="${download_tasks[idx]}"
          IMAGEID="${task_images[idx]}"
          IMAGEURL="${task_urls[idx]}"

          # Limit concurrent jobs to N
          while (( ${RUNNING_JOBS@P} >= N )); do
            wait -n || success=false
          done

          echo "Starting download for IMAGE: $IMAGEID from URL: $IMAGEURL"
          download_sbom_for_image "$PYXIS_IMAGE" "$IMAGEURL" > "${IMAGEID}.download.out" 2>&1 &
        done

        # Wait for remaining processes to finish
        while (( ${RUNNING_JOBS@P} > 0 )); do
          wait -n || success=false
        done
        echo "All SBOM download jobs completed"

        # Print outputs for all download jobs
        echo
        echo "Printing outputs for all download runs"
        for (( idx=0; idx < total; idx++ )); do
          img="${task_images[idx]}"
          echo "=== $img ==="
          cat "${img}.download.out"
          echo
        done

        if [ $success != "true" ]; then
          echo "ERROR: At least one download failed"
          exit 1
        fi

        # Verify we downloaded the expected number of unique SBOMs
        sbom_files=(*.json)
        SBOM_COUNT=${#sbom_files[@]}
        if [ "$SBOM_COUNT" != "$UNIQUE_IMAGES_COUNT" ]; then
          echo "ERROR: Expected to fetch sbom for $UNIQUE_IMAGES_COUNT unique images, but only $SBOM_COUNT were saved"
          exit 1
        fi
    - name: push-rpm-data-to-pyxis
      image:
        quay.io/konflux-ci/release-service-utils:fe734aa04168690e96f0a729f93845e7c70b7934
      computeResources:
        limits:
          memory: 512Mi
        requests:
          memory: 512Mi
          cpu: '1'
      volumeMounts:
        - name: pyxis-secret-vol
          mountPath: "/etc/secrets"
      script: |
        #!/usr/bin/env bash
        set -eu

        PYXIS_CERT="$(cat /etc/secrets/cert)"
        PYXIS_KEY="$(cat /etc/secrets/key)"

        if [[ "$(params.server)" == "production" ]]
        then
          export PYXIS_GRAPHQL_API="https://graphql-pyxis.api.redhat.com/graphql/"
        elif [[ "$(params.server)" == "stage" ]]
        then
          export PYXIS_GRAPHQL_API="https://graphql-pyxis.preprod.api.redhat.com/graphql/"
        elif [[ "$(params.server)" == "production-internal" ]]
        then
          export PYXIS_GRAPHQL_API="https://graphql.pyxis.engineering.redhat.com/graphql/"
        elif [[ "$(params.server)" == "stage-internal" ]]
        then
          export PYXIS_GRAPHQL_API="https://graphql.pyxis.stage.engineering.redhat.com/graphql/"
        else
          echo "Invalid server parameter. Only 'production','production-internal,'stage-internal' and 'stage' allowed."
          exit 1
        fi

        export PYXIS_CERT_PATH=/tmp/crt
        export PYXIS_KEY_PATH=/tmp/key
        echo "${PYXIS_CERT}" > $PYXIS_CERT_PATH
        echo "${PYXIS_KEY}" > $PYXIS_KEY_PATH

        PYXIS_FILE="$(params.dataDir)/$(params.pyxisJsonPath)"
        if [ ! -f "${PYXIS_FILE}" ] ; then
            echo "No valid pyxis file was provided."
            exit 1
        fi

        SBOM_PATH="/var/workdir/downloaded-sboms"
        cd "${SBOM_PATH}"

        # Create a list of all images to push to Pyxis (including duplicates)
        echo "Creating list of all images to push to Pyxis..."
        ALL_IMAGES_FILE="$(mktemp)"
        jq '
        # Flatten all pyxisImages from all components
        [.components[] | .pyxisImages[]] |
        map({
          imageId: .imageId,
          containerImage: .containerImage
        })' "${PYXIS_FILE}" > "${ALL_IMAGES_FILE}"

        TOTAL_IMAGES_TO_PUSH=$(jq '. | length' "${ALL_IMAGES_FILE}")
        echo "Will push RPM data for $TOTAL_IMAGES_TO_PUSH total images"

        N=$(params.concurrentLimit)  # The maximum number of images to be processed at once
        RUNNING_JOBS="\j" # Bash parameter for number of jobs currently running
        success=true
        echo "Starting RPM data upload for $TOTAL_IMAGES_TO_PUSH images in total. " \
          "Up to $N images will be uploaded at once..."

        for (( i=0; i < TOTAL_IMAGES_TO_PUSH; i++ )); do
          IMAGE_DATA=$(jq -c --argjson i "$i" '.[$i]' "${ALL_IMAGES_FILE}")
          IMAGEID=$(jq -r '.imageId' <<< "$IMAGE_DATA")

          # Find the corresponding SBOM file
          SBOM_FILE="${IMAGEID}.json"
          if [ ! -f "$SBOM_FILE" ]; then
            echo "ERROR: SBOM file $SBOM_FILE not found for image $IMAGEID"
            exit 1
          fi

          # Validate SBOM format (SPDX only)
          SPDX_VERSION=$(jq -r '.spdxVersion // empty' "$SBOM_FILE")
          if [ -z "$SPDX_VERSION" ]; then
            echo "Error: ${SBOM_FILE}: not a valid SPDX SBOM"
            exit 1
          fi

          # Limit concurrent jobs to N
          while (( ${RUNNING_JOBS@P} >= N )); do
            wait -n || success=false
          done

          echo "Uploading RPM data to Pyxis for IMAGE: $IMAGEID with SBOM: $SBOM_FILE using script: upload_rpm_data"
          upload_rpm_data --retry --image-id "$IMAGEID" --sbom-path "$SBOM_FILE" --verbose > "${IMAGEID}.out" 2>&1 &
        done

        # Wait for remaining processes to finish
        while (( ${RUNNING_JOBS@P} > 0 )); do
          wait -n || success=false
        done
        echo "All RPM data upload jobs completed"

        # Print outputs for all upload jobs
        echo
        echo "Printing outputs for all upload_rpm_data script runs"
        for FILE in *.json; do
          IMAGEID=$(echo "$FILE" | cut -d '.' -f 1)
          echo "=== $IMAGEID ==="
          cat "${IMAGEID}.out"
          echo
        done

        if [ $success != "true" ]; then
          echo "ERROR: At least one upload failed"
          exit 1
        fi
    - name: create-trusted-artifact
      computeResources:
        limits:
          memory: 128Mi
        requests:
          memory: 128Mi
          cpu: 250m
      ref:
        resolver: "git"
        params:
          - name: url
            value: "$(params.taskGitUrl)"
          - name: revision
            value: "$(params.taskGitRevision)"
          - name: pathInRepo
            value: stepactions/create-trusted-artifact/create-trusted-artifact.yaml
      params:
        - name: ociStorage
          value: $(params.ociStorage)
        - name: workDir
          value: $(params.dataDir)
        - name: sourceDataArtifact
          value: $(results.sourceDataArtifact.path)
